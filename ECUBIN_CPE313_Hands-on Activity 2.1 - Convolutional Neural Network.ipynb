{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "historic-trainer",
   "metadata": {},
   "source": [
    "# Activity 2.1 : Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-manner",
   "metadata": {},
   "source": [
    "#### Objective(s):\n",
    "\n",
    "This activity aims to introduce how to build a convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-transport",
   "metadata": {},
   "source": [
    "#### Intended Learning Outcomes (ILOs):\n",
    "* Demonstrate how to build and train convolutional neural network \n",
    "* Evaluate the accuracy and loss of the model using convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-providence",
   "metadata": {},
   "source": [
    "#### Resources:\n",
    "* Jupyter Notebook\n",
    "* CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-trail",
   "metadata": {},
   "source": [
    "#### Procedures\n",
    "Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stretch-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-seating",
   "metadata": {},
   "source": [
    "* Shuffle the data\n",
    "* Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "modular-springer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-carolina",
   "metadata": {},
   "source": [
    "Check the image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alleged-stephen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_train[444].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-twenty",
   "metadata": {},
   "source": [
    "Visualize one of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "positive-creation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(y_train[444])\n",
    "plt.imshow(x_train[444]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "animated-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-filing",
   "metadata": {},
   "source": [
    "Instead of classes described by an integer between 0-9 we have a vector with a 1 in the (Pythonic) 9th position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "genetic-centre",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[444]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-education",
   "metadata": {},
   "source": [
    "Convert to float and scale the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "familiar-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-smith",
   "metadata": {},
   "source": [
    "Build a CNN using Keras Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "understanding-milan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 16, 16, 32)        2432      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 6, 6, 32)          25632     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 6, 6, 32)          0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 3, 3, 32)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3, 3, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 288)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               147968    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181162 (707.66 KB)\n",
      "Trainable params: 181162 (707.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_1 = Sequential()\n",
    "\n",
    "\n",
    "## 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2)))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## 2x2 max pooling reduces to 3 x 3 x 32\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(Dropout(0.25))\n",
    "\n",
    "## Flatten turns 3x3x32 into 288x1\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(512))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(num_classes))\n",
    "model_1.add(Activation('softmax'))\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-portable",
   "metadata": {},
   "source": [
    "* Use batch size of 32\n",
    "* Initiate RMSprop optimizer\n",
    "* Train the model using RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "removed-memorial",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6821 - accuracy: 0.3859 - val_loss: 1.5000 - val_accuracy: 0.4823\n",
      "Epoch 2/15\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.4280 - accuracy: 0.4898 - val_loss: 1.2856 - val_accuracy: 0.5423\n",
      "Epoch 3/15\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.3458 - accuracy: 0.5221 - val_loss: 1.2919 - val_accuracy: 0.5523\n",
      "Epoch 4/15\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.3073 - accuracy: 0.5403 - val_loss: 1.3103 - val_accuracy: 0.5429\n",
      "Epoch 5/15\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.2896 - accuracy: 0.5498 - val_loss: 1.2620 - val_accuracy: 0.5676\n",
      "Epoch 6/15\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.2736 - accuracy: 0.5584 - val_loss: 1.1610 - val_accuracy: 0.5903\n",
      "Epoch 7/15\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.2752 - accuracy: 0.5600 - val_loss: 1.2107 - val_accuracy: 0.5874\n",
      "Epoch 8/15\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.2627 - accuracy: 0.5688 - val_loss: 1.1846 - val_accuracy: 0.5993\n",
      "Epoch 9/15\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.2594 - accuracy: 0.5720 - val_loss: 1.2828 - val_accuracy: 0.5563\n",
      "Epoch 10/15\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.2592 - accuracy: 0.5721 - val_loss: 1.2170 - val_accuracy: 0.5839\n",
      "Epoch 11/15\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.2646 - accuracy: 0.5728 - val_loss: 1.1632 - val_accuracy: 0.6009\n",
      "Epoch 12/15\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.2759 - accuracy: 0.5684 - val_loss: 1.2550 - val_accuracy: 0.5694\n",
      "Epoch 13/15\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.2713 - accuracy: 0.5712 - val_loss: 1.1755 - val_accuracy: 0.6155\n",
      "Epoch 14/15\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 1.2916 - accuracy: 0.5648 - val_loss: 1.3330 - val_accuracy: 0.5709\n",
      "Epoch 15/15\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.2975 - accuracy: 0.5666 - val_loss: 1.1785 - val_accuracy: 0.6037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28b986ca410>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "opt = keras.optimizers.RMSprop(lr=0.0005)\n",
    "\n",
    "\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_1.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=15,\n",
    "              validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-lambda",
   "metadata": {},
   "source": [
    "#### Supplementary Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-theater",
   "metadata": {},
   "source": [
    "* Build a more complicated model with the following pattern:\n",
    "Conv -> Conv -> MaxPool -> Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "\n",
    "* Use strides of 1 for all convolutional layers.\n",
    "\n",
    "* Write the number of parameters of your model  and compare it to the previous model\n",
    "\n",
    "* Train it for 5 epochs. Commpare the training time, loss and accuracy numbers (on both the training and validation sets)?\n",
    "\n",
    "* Use different structures and run times, and see how accurate your model can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be5e40e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 32)        2432      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 32)        25632     \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 14, 14, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 10, 10, 32)        25632     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 10, 10, 32)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 6, 6, 32)          25632     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 6, 6, 32)          0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 3, 3, 32)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 3, 3, 32)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 288)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               147968    \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 232426 (907.91 KB)\n",
      "Trainable params: 232426 (907.91 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "\n",
    "\n",
    "# Conv #1\n",
    "model_2.add(Conv2D(32, (5, 5), strides = 1, padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "# Conv #2\n",
    "model_2.add(Conv2D(32, (5, 5), strides = 1))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "## MaxPool #1\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "# Conv #3\n",
    "model_2.add(Conv2D(32, (5, 5), strides = 1))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "# Conv #4\n",
    "model_2.add(Conv2D(32, (5, 5), strides = 1))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "# MaxPool #2\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "## Flatten turns 3x3x32 into 288x1\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(512))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(num_classes))\n",
    "model_2.add(Activation('softmax'))\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a64fdf6",
   "metadata": {},
   "source": [
    "### Number of parameters of model_2 is 232,426 while model_1 has 181,162 total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b39ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 85s 54ms/step - loss: 1.7180 - accuracy: 0.3671 - val_loss: 1.4281 - val_accuracy: 0.4748\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.3909 - accuracy: 0.5039 - val_loss: 1.1999 - val_accuracy: 0.5749\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.2783 - accuracy: 0.5541 - val_loss: 1.1924 - val_accuracy: 0.5828\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.2252 - accuracy: 0.5784 - val_loss: 1.1920 - val_accuracy: 0.5985\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.2123 - accuracy: 0.5913 - val_loss: 1.1206 - val_accuracy: 0.6062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28ba2be46d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "opt = keras.optimizers.RMSprop(lr=0.0005)\n",
    "\n",
    "\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_2.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=5,\n",
    "              validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd7eba0",
   "metadata": {},
   "source": [
    "The training time per epoch is 84 seconds. The average loss for the training data is 1.35, while the validation data has an average loss of 1.27. Regarding accuracy, the validation data performs better than the training data. It starts at 0.5031 on the first epoch and increases to 0.5954 on the last epoch. In comparison, the training data starts at 0.3639 and increases to 0.5871 on the last epoch. However, these results from both the training and validation data suggest that the model's performance might not be optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7742255f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 32, 32, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 32, 32, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 16, 16, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 16, 16, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 16, 16, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 8, 8, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 8, 8, 128)         512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 8, 8, 128)         512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 4, 4, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               262272    \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 552362 (2.11 MB)\n",
      "Trainable params: 551466 (2.10 MB)\n",
      "Non-trainable params: 896 (3.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3 = Sequential()\n",
    "\n",
    "# Convolutional Layer\n",
    "model_3.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model_3.add(BatchNormalization())\n",
    "# Pooling layer\n",
    "model_3.add(MaxPool2D(pool_size=(2, 2)))\n",
    "# Dropout layers\n",
    "model_3.add(Dropout(0.25))\n",
    "\n",
    "model_3.add(Conv2D(filters=64, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(Conv2D(filters=64, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model_3.add(Dropout(0.25))\n",
    "\n",
    "model_3.add(Conv2D(filters=128, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(Conv2D(filters=128, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model_3.add(Dropout(0.25))\n",
    "\n",
    "model_3.add(Flatten())\n",
    "# model_3.add(Dropout(0.2))\n",
    "model_3.add(Dense(128, activation='relu'))\n",
    "model_3.add(Dropout(0.25))\n",
    "model_3.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69c4bbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1563/1563 [==============================] - 148s 94ms/step - loss: 1.5418 - accuracy: 0.4600 - val_loss: 1.2239 - val_accuracy: 0.5819\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 1.1553 - accuracy: 0.6082 - val_loss: 1.0724 - val_accuracy: 0.6427\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 1.0095 - accuracy: 0.6590 - val_loss: 0.9272 - val_accuracy: 0.6865\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.9259 - accuracy: 0.6922 - val_loss: 0.9782 - val_accuracy: 0.7131\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.8554 - accuracy: 0.7137 - val_loss: 0.7184 - val_accuracy: 0.7579\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.7991 - accuracy: 0.7310 - val_loss: 0.7340 - val_accuracy: 0.7614\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.7465 - accuracy: 0.7495 - val_loss: 1.0145 - val_accuracy: 0.7001\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.6918 - accuracy: 0.7642 - val_loss: 0.7320 - val_accuracy: 0.7628\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.6583 - accuracy: 0.7808 - val_loss: 0.6904 - val_accuracy: 0.7692\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.6100 - accuracy: 0.7930 - val_loss: 1.1400 - val_accuracy: 0.6812\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.5808 - accuracy: 0.8018 - val_loss: 0.7309 - val_accuracy: 0.7812\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.5418 - accuracy: 0.8171 - val_loss: 0.5948 - val_accuracy: 0.7997\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.5153 - accuracy: 0.8235 - val_loss: 0.5971 - val_accuracy: 0.8054\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 0.4879 - accuracy: 0.8345 - val_loss: 0.6640 - val_accuracy: 0.7894\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 0.4637 - accuracy: 0.8416 - val_loss: 0.5306 - val_accuracy: 0.8281\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 0.4383 - accuracy: 0.8480 - val_loss: 0.5623 - val_accuracy: 0.8223\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.4277 - accuracy: 0.8533 - val_loss: 0.5664 - val_accuracy: 0.8164\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 0.4114 - accuracy: 0.8593 - val_loss: 0.7298 - val_accuracy: 0.7768\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 0.4008 - accuracy: 0.8638 - val_loss: 0.7386 - val_accuracy: 0.7651\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 0.3841 - accuracy: 0.8687 - val_loss: 0.5177 - val_accuracy: 0.8368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28bcaa67cd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "opt = keras.optimizers.RMSprop(lr=0.0005)\n",
    "\n",
    "\n",
    "model_3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_3.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=20,\n",
    "              validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd24707",
   "metadata": {},
   "source": [
    "After trying different network structures, this is the best result I've got so far wherein last epoch, the training accuracy is 0.8687 while validation accuracy is 0.8368 and the training loss is 0.3841 and validation loss is 0.5177."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-still",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-jackson",
   "metadata": {},
   "source": [
    "In this activity, I learned about how convolutional neural networks (CNNs) function and their usefulness in image detection. I also gained experience in building and training CNNs. CNNs are powerful tools because they can automatically learn features from data, making them highly valuable for a wide range of applications in computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce24da1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
