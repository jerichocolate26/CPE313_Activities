{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFhMDyD-vXLs"
      },
      "source": [
        "NOTE: For the most up to date version of this notebook, please be sure to copy from this link:\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ByRi9d6_Yzu0nrEKArmLMLuMaZjYfygO#scrollTo=WgHANbxqWJPa)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgHANbxqWJPa"
      },
      "source": [
        "## **Training YOLOv3 object detection on a custom dataset**\n",
        "\n",
        "💡 Recommendation: [Open this blog post](https://blog.roboflow.ai/training-a-yolov3-object-detection-model-with-a-custom-dataset/) to continue.\n",
        "\n",
        "### **Overview**\n",
        "\n",
        "This notebook walks through how to train a YOLOv3 object detection model on your own dataset using Roboflow and Colab.\n",
        "\n",
        "In this specific example, we'll training an object detection model to recognize chess pieces in images. **To adapt this example to your own dataset, you only need to change one line of code in this notebook.**\n",
        "\n",
        "![Chess Example](https://i.imgur.com/nkjobw1.png)\n",
        "\n",
        "### **Our Data**\n",
        "\n",
        "Our dataset of 289 chess images (and 2894 annotations!) is hosted publicly on Roboflow [here](https://public.roboflow.ai/object-detection/chess-full).\n",
        "\n",
        "### **Our Model**\n",
        "\n",
        "We'll be training a YOLOv3 (You Only Look Once) model. This specific model is a one-shot learner, meaning each image only passes through the network once to make a prediction, which allows the architecture to be very performant, viewing up to 60 frames per second in predicting against video feeds.\n",
        "\n",
        "The GitHub repo containing the majority of the code we'll use is available [here](https://github.com/roboflow-ai/keras-yolo3.git).\n",
        "\n",
        "### **Training**\n",
        "\n",
        "Google Colab provides free GPU resources. Click \"Runtime\" → \"Change runtime type\" → Hardware Accelerator dropdown to \"GPU.\"\n",
        "\n",
        "Colab does have memory limitations, and notebooks must be open in your browser to run. Sessions automatically clear themselves after 24 hours.\n",
        "\n",
        "### **Inference**\n",
        "\n",
        "We'll leverage the `python_video.py` script to produce predictions. Arguments are specified below.\n",
        "\n",
        "It's recommended that you expand the left-hand panel to view this notebook's Table of contents, Code Snippets, and Files.\n",
        "\n",
        "![Expand Colab](https://i.imgur.com/r8kWzIv.png \"Click here\")\n",
        "\n",
        "Then, click \"Files.\" You'll see files appear here as we work through the notebook.\n",
        "\n",
        "\n",
        "### **About**\n",
        "\n",
        "[Roboflow](https://roboflow.ai) makes managing, preprocessing, augmenting, and versioning datasets for computer vision seamless.\n",
        "\n",
        "Developers reduce 50% of their boilerplate code when using Roboflow's workflow, save training time, and increase model reproducibility.\n",
        "\n",
        "#### ![Roboflow Workmark](https://i.imgur.com/WHFqYSJ.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHNPC6kwbKAL"
      },
      "source": [
        "## Setup our environment\n",
        "\n",
        "First, we'll install the version of Keras our YOLOv3 implementation calls for and verify it installs corrects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pyrwfpiiEkH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8bd6a0b-8bff-448e-ddf6-743808a3316f"
      },
      "source": [
        "# Get our kernel running\n",
        "print(\"Hello, Roboflow\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, Roboflow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRIj10jNhqH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c57c35b-7fb7-43a0-ba33-b5619bbed5b7"
      },
      "source": [
        "# Our YOLOv3 implementation calls for this Keras version\n",
        "!pip install keras==2.2.4"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras==2.2.4\n",
            "  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/312.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/312.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.5/312.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras==2.2.4) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.10/dist-packages (from keras==2.2.4) (1.11.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras==2.2.4) (1.16.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from keras==2.2.4) (6.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras==2.2.4) (3.9.0)\n",
            "Collecting keras-applications>=1.0.6 (from keras==2.2.4)\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from keras==2.2.4)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-preprocessing, keras-applications, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.2.4 keras-applications-1.0.8 keras-preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r788kvmKuD5O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d836d59-6a61-447a-dfaa-0d7e2b68e162"
      },
      "source": [
        "# use TF 1.x\n",
        "#%tensorflow_version 1.x\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMI-zNrrhmuG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f445217-0564-4dac-efd6-46ba2be59a3a"
      },
      "source": [
        "# Verify our version is correct\n",
        "!python -c 'import keras; print(keras.__version__)'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend.\n",
            "2024-04-01 11:45:19.200060: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-01 11:45:19.200123: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-01 11:45:19.201760: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-01 11:45:19.208923: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-01 11:45:20.166872: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/__init__.py\", line 3, in <module>\n",
            "    from . import utils\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/__init__.py\", line 27, in <module>\n",
            "    from .multi_gpu_utils import multi_gpu_model\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/multi_gpu_utils.py\", line 7, in <module>\n",
            "    from ..layers.merge import concatenate\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/layers/__init__.py\", line 4, in <module>\n",
            "    from ..engine.base_layer import Layer\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/__init__.py\", line 8, in <module>\n",
            "    from .training import Model\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 21, in <module>\n",
            "    from . import training_arrays\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training_arrays.py\", line 14, in <module>\n",
            "    from .. import callbacks as cbks\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\", line 20, in <module>\n",
            "    from collections import Iterable\n",
            "ImportError: cannot import name 'Iterable' from 'collections' (/usr/lib/python3.10/collections/__init__.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lweWDcTyVeLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f215d097-ef63-4d33-8397-757cd1471290"
      },
      "source": [
        "# Next, we'll grab all the code from our repository of interest\n",
        "!git clone https://github.com/roboflow-ai/keras-yolo3"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'keras-yolo3'...\n",
            "remote: Enumerating objects: 169, done.\u001b[K\n",
            "remote: Total 169 (delta 0), reused 0 (delta 0), pack-reused 169\u001b[K\n",
            "Receiving objects: 100% (169/169), 172.74 KiB | 951.00 KiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyPfLjFBbOAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1add7c0-5c58-4a2b-f72a-1cc4c26b28fb"
      },
      "source": [
        "# here's what we cloned (also, see \"Files\" in the left-hand Colab pane)\n",
        "%ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mkeras-yolo3\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adwdKfxBVlom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0267e785-6279-4bf9-f02d-c3bb6bb854d2"
      },
      "source": [
        "# change directory to the repo we cloned\n",
        "%cd keras-yolo3/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/keras-yolo3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6DNWhOEbGB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "267ad10d-a8b0-4a17-9a88-eb002a36a4f1"
      },
      "source": [
        "# show the contents of our repo\n",
        "%ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coco_annotation.py  \u001b[0m\u001b[01;34mfont\u001b[0m/      \u001b[01;34mmodel_data\u001b[0m/          train.py           \u001b[01;34myolo3\u001b[0m/      yolov3-tiny.cfg\n",
            "convert.py          kmeans.py  README.md            Tutorial.ipynb     yolo.py     yolo_video.py\n",
            "darknet53.cfg       LICENSE    train_bottleneck.py  voc_annotation.py  yolov3.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I--RqDmpwqmv"
      },
      "source": [
        "## Get our training data from Roboflow\n",
        "\n",
        "Next, we need to add our data from Roboflow into our environment.\n",
        "\n",
        "Our dataset, with annotations, is [here](https://public.roboflow.ai/object-detection/chess-full).\n",
        "\n",
        "Here's how to bring those images from Roboflow to Colab:\n",
        "\n",
        "1. Visit this [link](https://public.roboflow.ai/object-detection/chess-full).\n",
        "2. Click the \"416x416auto-orient\" under Downloads.\n",
        "3. On the dataset detail page, select \"Download\" in the upper right-hand corner.\n",
        "4. If you are not signed in, you will be prompted to create a free account (sign in with GitHub or email), and redirected to the dataset page to Download.\n",
        "5. On the download popup, select the YOLOv3 Keras option **and** the \"Show download `code`\".\n",
        "6. Copy the code snippet Roboflow generates for you, and paste it in the next cell.\n",
        "\n",
        "This is the download menu you want (from step 5):\n",
        "#### ![Download Menu](https://i.imgur.com/KW2PyQO.png)\n",
        "\n",
        "The top code snippet is the one you want to copy (from step 6) and paste in the next notebook cell:\n",
        "### ![Code Snippet](https://i.imgur.com/qzJckWR.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AmSSTFFWud7"
      },
      "source": [
        "**This cell below is only one you need to change to have YOLOv3 train on your own Roboflow dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nclkjonbT25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0504f4a2-5b39-42b4-a7bc-6bcb270b9806"
      },
      "source": [
        "# Paste Roboflow code from snippet here from above to here! eg !curl -L https://app.roboflow.ai/ds/eOSXbt7KWu?key=YOURKEY | jar -x\n",
        "!curl -L https://app.roboflow.ai/ds/hGaXfr3E2f?key=uvY28v4oIn > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    68  100    68    0     0    488      0 --:--:-- --:--:-- --:--:--   489\n",
            "100   906  100   906    0     0   2358      0 --:--:-- --:--:-- --:--:--  2358\n",
            "100 5362k  100 5362k    0     0  5858k      0 --:--:-- --:--:-- --:--:-- 15.4M\n",
            "Archive:  roboflow.zip\n",
            " extracting: README.dataset.txt      \n",
            " extracting: README.roboflow.txt     \n",
            "   creating: test/\n",
            " extracting: test/-7-_jpg.rf.cf8e3c211a85d6349bcb88dc34412ff2.jpg  \n",
            " extracting: test/-7-_jpg.rf.e04a163c1db6c99178933078ceb37e81.jpg  \n",
            " extracting: test/-_-_jpg.rf.463adc348eae20a9a54bc0a2377f183c.jpg  \n",
            " extracting: test/-_-_jpg.rf.b186c55468a6e44c0b913e0a70245773.jpg  \n",
            " extracting: test/-_jpg.rf.2d6f5ca77ea094c690fe9f828b757514.jpg  \n",
            " extracting: test/1616164550_1-p-sobaka-khatiko-sobaka-foto-1_jpg.rf.7809493099f6ded68be3eafc0fa4c8c2.jpg  \n",
            " extracting: test/1616164550_1-p-sobaka-khatiko-sobaka-foto-1_jpg.rf.8eec79f91cb38e661d1a2a1b82394a45.jpg  \n",
            " extracting: test/25-Dogs-That-Are-So-Big-You-Won-t-Believe-They-re-Real_jpg.rf.b29e23b20d24ae5f2fcb64e6b79ccc21.jpg  \n",
            " extracting: test/304d0f28f7fc2cd20f275cc29904db38_jpg.rf.364e337c775c6c2d9cdb8e2e4d0ddc4f.jpg  \n",
            " extracting: test/304d0f28f7fc2cd20f275cc29904db38_jpg.rf.462763f939df3093c018d3ce059a0c59.jpg  \n",
            " extracting: test/338b764fe3145d233e2e92fe9a760a87_jpeg_jpg.rf.02f3e51e83f196478948d2b8661f3aad.jpg  \n",
            " extracting: test/338b764fe3145d233e2e92fe9a760a87_jpeg_jpg.rf.78c8ec233c9de00ba85b8e5aead02733.jpg  \n",
            " extracting: test/573c3aa8b94a2908ef236e463d6bf087_jpeg_jpg.rf.5ab32a629b10d1124cee2388c57237d8.jpg  \n",
            " extracting: test/573c3aa8b94a2908ef236e463d6bf087_jpeg_jpg.rf.f4e841da0da3f62b6718e65734ff1c71.jpg  \n",
            " extracting: test/_annotations.txt   \n",
            " extracting: test/_classes.txt       \n",
            " extracting: test/b5f7dcfff5743b9d68c714ed3a6397fa_jpg.rf.8c577afa3cdae8029a45802dd9633d48.jpg  \n",
            " extracting: test/b5f7dcfff5743b9d68c714ed3a6397fa_jpg.rf.c0e326ab50c84fbffab8cdbc24a35263.jpg  \n",
            " extracting: test/bladxaund-zabor-ushi-morda-lapa_jpg.rf.bf0bb42290b3e677d972d3e5767580fe.jpg  \n",
            "   creating: train/\n",
            " extracting: train/-1-_jpg.rf.5084b0e42b1f389cd7ae95462e9a2096.jpg  \n",
            " extracting: train/-1-_jpg.rf.8f1b031eaaade9e3d97e627fe3e03547.jpg  \n",
            " extracting: train/-1-_jpg.rf.8fb3786a3b639248948a14f59f2122e9.jpg  \n",
            " extracting: train/-1-_jpg.rf.ae76805c47358e304904cb370a435087.jpg  \n",
            " extracting: train/-10-_jpg.rf.38c4f441b5a00425e8afd8c42eb8cb48.jpg  \n",
            " extracting: train/-10-_jpg.rf.39b13fbf75e04d7cd6ff78b20d299313.jpg  \n",
            " extracting: train/-10-_jpg.rf.769246e6deb9fd2e05d51fced37858d3.jpg  \n",
            " extracting: train/-10-_jpg.rf.7aab48417bcc4d93bd7ff11b4097ff12.jpg  \n",
            " extracting: train/-10-_jpg.rf.ba710cc6e0b8b56d5ebc4514258d9c78.jpg  \n",
            " extracting: train/-10-_jpg.rf.c9bd0765193031a228e5d3946bee57af.jpg  \n",
            " extracting: train/-12-_jpg.rf.1199e54a4dc9c516521a49b36dd704e4.jpg  \n",
            " extracting: train/-12-_jpg.rf.3bb0bbe23bbb712d6ee1a9ff391ac236.jpg  \n",
            " extracting: train/-12-_jpg.rf.476bbe5b953762e2f70342c9f7e655a1.jpg  \n",
            " extracting: train/-12-_jpg.rf.70d55d0619a78d3e85d5a168ff28a54e.jpg  \n",
            " extracting: train/-12-_jpg.rf.99f754321f69bc4835f483b7bbafe011.jpg  \n",
            " extracting: train/-12-_jpg.rf.e73077cf8ed380a901a1bba840180979.jpg  \n",
            " extracting: train/-13-_jpg.rf.07bf26fb023b0a2b91a93b6c1044e76b.jpg  \n",
            " extracting: train/-13-_jpg.rf.0f2ac8b5d9e1d189750f15d6187c90bb.jpg  \n",
            " extracting: train/-13-_jpg.rf.703d1427e73cff4bdfc9b5a285d8676b.jpg  \n",
            " extracting: train/-13-_jpg.rf.ea5f69bff8ec457397a58c2e19f78679.jpg  \n",
            " extracting: train/-13-_jpg.rf.f257bf14094bcbf372ace1df7bd5ed1b.jpg  \n",
            " extracting: train/-13-_jpg.rf.fe980469c1077c50f69e91a65f7d72de.jpg  \n",
            " extracting: train/-14-_jpg.rf.0d82da8204df1a9ce3225d6d3b0b68ea.jpg  \n",
            " extracting: train/-14-_jpg.rf.2a26d3fcb5cfc2260ab2b65e4041043a.jpg  \n",
            " extracting: train/-14-_jpg.rf.358c0d66df6913dd60ec940c2711723b.jpg  \n",
            " extracting: train/-14-_jpg.rf.7e1229a3a5c8d597d77bea3a47ee8091.jpg  \n",
            " extracting: train/-14-_jpg.rf.a26300a0777670976603c51453d32dad.jpg  \n",
            " extracting: train/-14-_jpg.rf.f7c11db857b24dc6d5273e74fdf8f625.jpg  \n",
            " extracting: train/-15-_jpg.rf.21d3161a6f885e1e0fc2abdbc7bdafbf.jpg  \n",
            " extracting: train/-15-_jpg.rf.2d1c6716c8e4b85313f65bf1f335e9a4.jpg  \n",
            " extracting: train/-15-_jpg.rf.b3feaceed22c60d8bdb195f36a6b0b7d.jpg  \n",
            " extracting: train/-15-_jpg.rf.f21a26de13377b735c278b7e0f26cf82.jpg  \n",
            " extracting: train/-17-_jpg.rf.0e3f200c10ddf8823f21b3e93be92111.jpg  \n",
            " extracting: train/-17-_jpg.rf.a4f342ef407395ecd3fee7c24825111e.jpg  \n",
            " extracting: train/-17-_jpg.rf.c2cbd4b5eb99805da01823e7b16eb6ae.jpg  \n",
            " extracting: train/-17-_jpg.rf.ec8b081adcd67efdb4fc592654c57106.jpg  \n",
            " extracting: train/-19-_jpg.rf.611b53a6a6646e3bd1baf2f395d56576.jpg  \n",
            " extracting: train/-19-_jpg.rf.86446a437d406800d544f6cc9779a597.jpg  \n",
            " extracting: train/-19-_jpg.rf.9861103873e1650a94777d0d312aad32.jpg  \n",
            " extracting: train/-19-_jpg.rf.ace7b293c94af3749c3ea8b443dec91e.jpg  \n",
            " extracting: train/-19-_jpg.rf.be46d8c3dc6db1d791c3983408095f8d.jpg  \n",
            " extracting: train/-19-_jpg.rf.d750e37d6ad5171ad2e240e4056d8772.jpg  \n",
            " extracting: train/-20-_jpg.rf.1c3ee4ef815987d5945d09e1ac70e049.jpg  \n",
            " extracting: train/-20-_jpg.rf.892b30db82ff0051fb1089d1c9ae20ef.jpg  \n",
            " extracting: train/-20-_jpg.rf.9e1e2924eaf3489a44c2b25033a011a5.jpg  \n",
            " extracting: train/-20-_jpg.rf.b59849675301292a736d576fa22809ec.jpg  \n",
            " extracting: train/-21-_jpg.rf.11ca4af2ba78c6c3e274da0b2f6f2e15.jpg  \n",
            " extracting: train/-21-_jpg.rf.25803a1ca3a7b6804b2a872c4a254262.jpg  \n",
            " extracting: train/-21-_jpg.rf.2e94124d7c3b29800493102d95551940.jpg  \n",
            " extracting: train/-21-_jpg.rf.34de628376df5e0e682a1ec6baa2d315.jpg  \n",
            " extracting: train/-21-_jpg.rf.9fc3810d3450d79ba3109848c56a3754.jpg  \n",
            " extracting: train/-21-_jpg.rf.ea6821cc7f29092a132de26195a9a12a.jpg  \n",
            " extracting: train/-22-_jpg.rf.01ba7e9b01f37a9a3be4a40eaf57d0ec.jpg  \n",
            " extracting: train/-22-_jpg.rf.702d0bb1e22165ea0f0f438e71f01fbe.jpg  \n",
            " extracting: train/-22-_jpg.rf.7df394f2721f0ce3023ce55fc125147e.jpg  \n",
            " extracting: train/-22-_jpg.rf.934d61c472620169f2b340058558db3d.jpg  \n",
            " extracting: train/-22-_jpg.rf.d0435464f4e0c38f962564519c68234a.jpg  \n",
            " extracting: train/-22-_jpg.rf.dbe4aef286f80040fd664207222a67c0.jpg  \n",
            " extracting: train/-26-_jpg.rf.53aa40bd945be5a558cfc470888097b9.jpg  \n",
            " extracting: train/-26-_jpg.rf.6ade30d9c1233ec52ad38c50e8e231f7.jpg  \n",
            " extracting: train/-28-_jpg.rf.061988628c0cb5942e846eb7b9af732c.jpg  \n",
            " extracting: train/-28-_jpg.rf.57881eb5d667e8361e3598747187e598.jpg  \n",
            " extracting: train/-28-_png_jpg.rf.00ba3535e3306323756a3a3895848cf7.jpg  \n",
            " extracting: train/-28-_png_jpg.rf.bb0217f0165f42f341e3306b36030859.jpg  \n",
            " extracting: train/-3-_jpg.rf.21976831bf2b865f53188c37c2aca506.jpg  \n",
            " extracting: train/-3-_jpg.rf.6f9c3b673ffa1a7513fa440d0f0e0908.jpg  \n",
            " extracting: train/-8-_jpg.rf.77cdfea2b90887db3e8d759578218f37.jpg  \n",
            " extracting: train/-8-_jpg.rf.af66bcae113e56911727ecb70ecc7dd5.jpg  \n",
            " extracting: train/-9-_jpg.rf.a1261b03dbbf7ac6009aa81d9a5da2db.jpg  \n",
            " extracting: train/-9-_jpg.rf.e12408303daf08a4747e536d3bfce53c.jpg  \n",
            " extracting: train/-_jpg.rf.44f64cf1df4698493bbeb0b649e3040c.jpg  \n",
            " extracting: train/-_jpg.rf.4a603b6ef7557b63b8772607e3c19b77.jpg  \n",
            " extracting: train/-_jpg.rf.55010bcdb855e9475873e4f349abc096.jpg  \n",
            " extracting: train/-_jpg.rf.9e6169635621fad4a4aadaad90207bbb.jpg  \n",
            " extracting: train/-_jpg.rf.b222245267d5d8af577ee779a1037805.jpg  \n",
            " extracting: train/-_jpg.rf.d8097b46ee90ab3990f6b2b4a361a668.jpg  \n",
            " extracting: train/-_jpg.rf.dd29a7c5e1959d4009101784297ac111.jpg  \n",
            " extracting: train/-_jpg.rf.f2bff597fbf955e4aef8f157437749db.jpg  \n",
            " extracting: train/-_png_jpg.rf.71bdc47f71765e5112599bb19e1eed2d.jpg  \n",
            " extracting: train/-_png_jpg.rf.f0fca57b97dfd469188973f9a07b5756.jpg  \n",
            " extracting: train/_annotations.txt  \n",
            " extracting: train/_classes.txt      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izGzSaeJzqAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f7f0e7-7daa-4324-d92a-b1ec362228d2"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coco_annotation.py  LICENSE              \u001b[0m\u001b[01;34mtest\u001b[0m/                voc_annotation.py  yolo_video.py\n",
            "convert.py          \u001b[01;34mmodel_data\u001b[0m/          \u001b[01;34mtrain\u001b[0m/               \u001b[01;34myolo3\u001b[0m/\n",
            "darknet53.cfg       README.dataset.txt   train_bottleneck.py  yolo.py\n",
            "\u001b[01;34mfont\u001b[0m/               README.md            train.py             yolov3.cfg\n",
            "kmeans.py           README.roboflow.txt  Tutorial.ipynb       yolov3-tiny.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1PagPopmUIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43043d58-63a1-4e23-ea6d-c2e0de799add"
      },
      "source": [
        "# change directory into our export folder from Roboflow\n",
        "%cd train"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/keras-yolo3/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ372c7gWN_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c59045e2-6917-4dda-82f5-5de978dada59"
      },
      "source": [
        "# show what came with the Roboflow export\n",
        "%ls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-10-_jpg.rf.38c4f441b5a00425e8afd8c42eb8cb48.jpg\n",
            "-10-_jpg.rf.39b13fbf75e04d7cd6ff78b20d299313.jpg\n",
            "-10-_jpg.rf.769246e6deb9fd2e05d51fced37858d3.jpg\n",
            "-10-_jpg.rf.7aab48417bcc4d93bd7ff11b4097ff12.jpg\n",
            "-10-_jpg.rf.ba710cc6e0b8b56d5ebc4514258d9c78.jpg\n",
            "-10-_jpg.rf.c9bd0765193031a228e5d3946bee57af.jpg\n",
            "-12-_jpg.rf.1199e54a4dc9c516521a49b36dd704e4.jpg\n",
            "-12-_jpg.rf.3bb0bbe23bbb712d6ee1a9ff391ac236.jpg\n",
            "-12-_jpg.rf.476bbe5b953762e2f70342c9f7e655a1.jpg\n",
            "-12-_jpg.rf.70d55d0619a78d3e85d5a168ff28a54e.jpg\n",
            "-12-_jpg.rf.99f754321f69bc4835f483b7bbafe011.jpg\n",
            "-12-_jpg.rf.e73077cf8ed380a901a1bba840180979.jpg\n",
            "-13-_jpg.rf.07bf26fb023b0a2b91a93b6c1044e76b.jpg\n",
            "-13-_jpg.rf.0f2ac8b5d9e1d189750f15d6187c90bb.jpg\n",
            "-13-_jpg.rf.703d1427e73cff4bdfc9b5a285d8676b.jpg\n",
            "-13-_jpg.rf.ea5f69bff8ec457397a58c2e19f78679.jpg\n",
            "-13-_jpg.rf.f257bf14094bcbf372ace1df7bd5ed1b.jpg\n",
            "-13-_jpg.rf.fe980469c1077c50f69e91a65f7d72de.jpg\n",
            "-14-_jpg.rf.0d82da8204df1a9ce3225d6d3b0b68ea.jpg\n",
            "-14-_jpg.rf.2a26d3fcb5cfc2260ab2b65e4041043a.jpg\n",
            "-14-_jpg.rf.358c0d66df6913dd60ec940c2711723b.jpg\n",
            "-14-_jpg.rf.7e1229a3a5c8d597d77bea3a47ee8091.jpg\n",
            "-14-_jpg.rf.a26300a0777670976603c51453d32dad.jpg\n",
            "-14-_jpg.rf.f7c11db857b24dc6d5273e74fdf8f625.jpg\n",
            "-15-_jpg.rf.21d3161a6f885e1e0fc2abdbc7bdafbf.jpg\n",
            "-15-_jpg.rf.2d1c6716c8e4b85313f65bf1f335e9a4.jpg\n",
            "-15-_jpg.rf.b3feaceed22c60d8bdb195f36a6b0b7d.jpg\n",
            "-15-_jpg.rf.f21a26de13377b735c278b7e0f26cf82.jpg\n",
            "-17-_jpg.rf.0e3f200c10ddf8823f21b3e93be92111.jpg\n",
            "-17-_jpg.rf.a4f342ef407395ecd3fee7c24825111e.jpg\n",
            "-17-_jpg.rf.c2cbd4b5eb99805da01823e7b16eb6ae.jpg\n",
            "-17-_jpg.rf.ec8b081adcd67efdb4fc592654c57106.jpg\n",
            "-19-_jpg.rf.611b53a6a6646e3bd1baf2f395d56576.jpg\n",
            "-19-_jpg.rf.86446a437d406800d544f6cc9779a597.jpg\n",
            "-19-_jpg.rf.9861103873e1650a94777d0d312aad32.jpg\n",
            "-19-_jpg.rf.ace7b293c94af3749c3ea8b443dec91e.jpg\n",
            "-19-_jpg.rf.be46d8c3dc6db1d791c3983408095f8d.jpg\n",
            "-19-_jpg.rf.d750e37d6ad5171ad2e240e4056d8772.jpg\n",
            "-1-_jpg.rf.5084b0e42b1f389cd7ae95462e9a2096.jpg\n",
            "-1-_jpg.rf.8f1b031eaaade9e3d97e627fe3e03547.jpg\n",
            "-1-_jpg.rf.8fb3786a3b639248948a14f59f2122e9.jpg\n",
            "-1-_jpg.rf.ae76805c47358e304904cb370a435087.jpg\n",
            "-20-_jpg.rf.1c3ee4ef815987d5945d09e1ac70e049.jpg\n",
            "-20-_jpg.rf.892b30db82ff0051fb1089d1c9ae20ef.jpg\n",
            "-20-_jpg.rf.9e1e2924eaf3489a44c2b25033a011a5.jpg\n",
            "-20-_jpg.rf.b59849675301292a736d576fa22809ec.jpg\n",
            "-21-_jpg.rf.11ca4af2ba78c6c3e274da0b2f6f2e15.jpg\n",
            "-21-_jpg.rf.25803a1ca3a7b6804b2a872c4a254262.jpg\n",
            "-21-_jpg.rf.2e94124d7c3b29800493102d95551940.jpg\n",
            "-21-_jpg.rf.34de628376df5e0e682a1ec6baa2d315.jpg\n",
            "-21-_jpg.rf.9fc3810d3450d79ba3109848c56a3754.jpg\n",
            "-21-_jpg.rf.ea6821cc7f29092a132de26195a9a12a.jpg\n",
            "-22-_jpg.rf.01ba7e9b01f37a9a3be4a40eaf57d0ec.jpg\n",
            "-22-_jpg.rf.702d0bb1e22165ea0f0f438e71f01fbe.jpg\n",
            "-22-_jpg.rf.7df394f2721f0ce3023ce55fc125147e.jpg\n",
            "-22-_jpg.rf.934d61c472620169f2b340058558db3d.jpg\n",
            "-22-_jpg.rf.d0435464f4e0c38f962564519c68234a.jpg\n",
            "-22-_jpg.rf.dbe4aef286f80040fd664207222a67c0.jpg\n",
            "-26-_jpg.rf.53aa40bd945be5a558cfc470888097b9.jpg\n",
            "-26-_jpg.rf.6ade30d9c1233ec52ad38c50e8e231f7.jpg\n",
            "-28-_jpg.rf.061988628c0cb5942e846eb7b9af732c.jpg\n",
            "-28-_jpg.rf.57881eb5d667e8361e3598747187e598.jpg\n",
            "-28-_png_jpg.rf.00ba3535e3306323756a3a3895848cf7.jpg\n",
            "-28-_png_jpg.rf.bb0217f0165f42f341e3306b36030859.jpg\n",
            "-3-_jpg.rf.21976831bf2b865f53188c37c2aca506.jpg\n",
            "-3-_jpg.rf.6f9c3b673ffa1a7513fa440d0f0e0908.jpg\n",
            "-8-_jpg.rf.77cdfea2b90887db3e8d759578218f37.jpg\n",
            "-8-_jpg.rf.af66bcae113e56911727ecb70ecc7dd5.jpg\n",
            "-9-_jpg.rf.a1261b03dbbf7ac6009aa81d9a5da2db.jpg\n",
            "-9-_jpg.rf.e12408303daf08a4747e536d3bfce53c.jpg\n",
            "_annotations.txt\n",
            "_classes.txt\n",
            "-_jpg.rf.44f64cf1df4698493bbeb0b649e3040c.jpg\n",
            "-_jpg.rf.4a603b6ef7557b63b8772607e3c19b77.jpg\n",
            "-_jpg.rf.55010bcdb855e9475873e4f349abc096.jpg\n",
            "-_jpg.rf.9e6169635621fad4a4aadaad90207bbb.jpg\n",
            "-_jpg.rf.b222245267d5d8af577ee779a1037805.jpg\n",
            "-_jpg.rf.d8097b46ee90ab3990f6b2b4a361a668.jpg\n",
            "-_jpg.rf.dd29a7c5e1959d4009101784297ac111.jpg\n",
            "-_jpg.rf.f2bff597fbf955e4aef8f157437749db.jpg\n",
            "-_png_jpg.rf.71bdc47f71765e5112599bb19e1eed2d.jpg\n",
            "-_png_jpg.rf.f0fca57b97dfd469188973f9a07b5756.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUWFxHW_mjlT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f77e47d-6d65-4842-8046-26a5e826f066"
      },
      "source": [
        "# move everything from the Roboflow export to the root of our keras-yolo3 folder\n",
        "%mv * ../"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: invalid option -- '1'\n",
            "Try 'mv --help' for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "200_8-VImWmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "502672bc-1ee6-494a-bd13-726572de0371"
      },
      "source": [
        "# change directory back to our\n",
        "%cd .."
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/keras-yolo3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQASf1hzmxE7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf68fbcd-48d1-4e34-8b7c-8b56513dfa8f"
      },
      "source": [
        "# show that all our images, _annotations.txt, and _classes.txt made it to our root directory\n",
        "%ls"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coco_annotation.py  LICENSE              \u001b[0m\u001b[01;34mtest\u001b[0m/                voc_annotation.py  yolo_video.py\n",
            "convert.py          \u001b[01;34mmodel_data\u001b[0m/          \u001b[01;34mtrain\u001b[0m/               \u001b[01;34myolo3\u001b[0m/\n",
            "darknet53.cfg       README.dataset.txt   train_bottleneck.py  yolo.py\n",
            "\u001b[01;34mfont\u001b[0m/               README.md            train.py             yolov3.cfg\n",
            "kmeans.py           README.roboflow.txt  Tutorial.ipynb       yolov3-tiny.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvzqgP92W7bt"
      },
      "source": [
        "## Set up and train our model\n",
        "\n",
        "Next, we'll download pre-trained weighs weights from DarkNet, set up our YOLOv3 architecture with those pre-trained weights, and initiate training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJzW08g2VlwD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c7b1e6-fa2a-4c93-b11c-a12429c74caa"
      },
      "source": [
        "# download our DarkNet weights\n",
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-01 11:46:42--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 162.0.215.52\n",
            "Connecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘yolov3.weights’\n",
            "\n",
            "yolov3.weights      100%[===================>] 236.52M  20.7MB/s    in 15s     \n",
            "\n",
            "2024-04-01 11:46:57 (16.0 MB/s) - ‘yolov3.weights’ saved [248007048/248007048]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mub8GJMBVluA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f4b560a-ff9c-467d-a3a2-e606b2dc012c"
      },
      "source": [
        "# call a Python script to set up our architecture with downloaded pre-trained weights\n",
        "!python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend.\n",
            "2024-04-01 11:47:01.011314: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-01 11:47:01.011360: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-01 11:47:01.012635: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-01 11:47:01.019545: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-01 11:47:01.959534: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/keras-yolo3/convert.py\", line 14, in <module>\n",
            "    from keras import backend as K\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/__init__.py\", line 3, in <module>\n",
            "    from . import utils\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/__init__.py\", line 27, in <module>\n",
            "    from .multi_gpu_utils import multi_gpu_model\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/multi_gpu_utils.py\", line 7, in <module>\n",
            "    from ..layers.merge import concatenate\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/layers/__init__.py\", line 4, in <module>\n",
            "    from ..engine.base_layer import Layer\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/__init__.py\", line 8, in <module>\n",
            "    from .training import Model\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 21, in <module>\n",
            "    from . import training_arrays\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training_arrays.py\", line 14, in <module>\n",
            "    from .. import callbacks as cbks\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\", line 20, in <module>\n",
            "    from collections import Iterable\n",
            "ImportError: cannot import name 'Iterable' from 'collections' (/usr/lib/python3.10/collections/__init__.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEDHwJ36YyXA"
      },
      "source": [
        "Below, we'll call a \"self-contained\" Python script that initiates training our model on our custom dataset.\n",
        "\n",
        "Pay notable attention to:\n",
        "- setting the paths for our `annotation_path`, `classes_path`, `class_names`. If you move the Roboflow data location, you'll need to update these.\n",
        "- `val_split` dictates the size of our training data relative to our taining data\n",
        "- `lr=1e-3` to set the learning rate of the model. Smaller optimizes more slowly but potentially more precisely.\n",
        "- `batch_size` for the number of images trained per batch\n",
        "-  `epoch` inside `model.fit_generator()` sets the number training epochs to increase/decrease training examples (and time)\n",
        "\n",
        "Consider reading the YOLOv3 paper [here](https://pjreddie.com/media/files/papers/YOLOv3.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hBFndz8VeI6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "ad77b01a-df40-49a0-ebd4-a221f8cad60e"
      },
      "source": [
        "\"\"\"\n",
        "Self-contained Python script to train YOLOv3 on your own dataset\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.layers import Input, Lambda\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n",
        "from yolo3.utils import get_random_data\n",
        "\n",
        "\n",
        "def _main():\n",
        "    annotation_path = '_annotations.txt'  # path to Roboflow data annotations\n",
        "    log_dir = 'logs/000/'                 # where we're storing our logs\n",
        "    classes_path = '_classes.txt'         # path to Roboflow class names\n",
        "    anchors_path = 'model_data/yolo_anchors.txt'\n",
        "    class_names = get_classes(classes_path)\n",
        "    print(\"-------------------CLASS NAMES-------------------\")\n",
        "    print(class_names)\n",
        "    print(\"-------------------CLASS NAMES-------------------\")\n",
        "    num_classes = len(class_names)\n",
        "    anchors = get_anchors(anchors_path)\n",
        "\n",
        "    input_shape = (416,416) # multiple of 32, hw\n",
        "\n",
        "    is_tiny_version = len(anchors)==6 # default setting\n",
        "    if is_tiny_version:\n",
        "        model = create_tiny_model(input_shape, anchors, num_classes,\n",
        "            freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')\n",
        "    else:\n",
        "        model = create_model(input_shape, anchors, num_classes,\n",
        "            freeze_body=2, weights_path='model_data/yolo.h5') # make sure you know what you freeze\n",
        "\n",
        "    logging = TensorBoard(log_dir=log_dir)\n",
        "    checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
        "        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
        "\n",
        "    val_split = 0.2 # set the size of the validation set\n",
        "    with open(annotation_path) as f:\n",
        "        lines = f.readlines()\n",
        "    np.random.seed(10101)\n",
        "    np.random.shuffle(lines)\n",
        "    np.random.seed(None)\n",
        "    num_val = int(len(lines)*val_split)\n",
        "    num_train = len(lines) - num_val\n",
        "\n",
        "    # Train with frozen layers first, to get a stable loss.\n",
        "    # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n",
        "    if True:\n",
        "        model.compile(optimizer=Adam(lr=1e-3), loss={\n",
        "            # use custom yolo_loss Lambda layer.\n",
        "            'yolo_loss': lambda y_true, y_pred: y_pred})\n",
        "\n",
        "        batch_size = 32\n",
        "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
        "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
        "                steps_per_epoch=max(1, num_train//batch_size),\n",
        "                validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
        "                validation_steps=max(1, num_val//batch_size),\n",
        "                epochs=500,\n",
        "                initial_epoch=0,\n",
        "                callbacks=[logging, checkpoint])\n",
        "        model.save_weights(log_dir + 'trained_weights_stage_1.h5')\n",
        "\n",
        "    # Unfreeze and continue training, to fine-tune.\n",
        "    # Train longer if the result is not good.\n",
        "    if True:\n",
        "        for i in range(len(model.layers)):\n",
        "            model.layers[i].trainable = True\n",
        "        model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
        "        print('Unfreeze all of the layers.')\n",
        "\n",
        "        batch_size = 32 # note that more GPU memory is required after unfreezing the body\n",
        "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
        "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
        "            steps_per_epoch=max(1, num_train//batch_size),\n",
        "            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
        "            validation_steps=max(1, num_val//batch_size),\n",
        "            epochs=100,\n",
        "            initial_epoch=50,\n",
        "            callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n",
        "        model.save_weights(log_dir + 'trained_weights_final.h5')\n",
        "\n",
        "    # Further training if needed.\n",
        "\n",
        "\n",
        "def get_classes(classes_path):\n",
        "    '''loads the classes'''\n",
        "    with open(classes_path) as f:\n",
        "        class_names = f.readlines()\n",
        "    class_names = [c.strip() for c in class_names]\n",
        "    return class_names\n",
        "\n",
        "def get_anchors(anchors_path):\n",
        "    '''loads the anchors from a file'''\n",
        "    with open(anchors_path) as f:\n",
        "        anchors = f.readline()\n",
        "    anchors = [float(x) for x in anchors.split(',')]\n",
        "    return np.array(anchors).reshape(-1, 2)\n",
        "\n",
        "\n",
        "def create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
        "            weights_path='model_data/yolo.h5'):\n",
        "    '''create the training model'''\n",
        "    K.clear_session() # get a new session\n",
        "    image_input = Input(shape=(None, None, 3))\n",
        "    h, w = input_shape\n",
        "    num_anchors = len(anchors)\n",
        "\n",
        "    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n",
        "        num_anchors//3, num_classes+5)) for l in range(3)]\n",
        "\n",
        "    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n",
        "    print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
        "\n",
        "    if load_pretrained:\n",
        "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
        "        print('Load weights {}.'.format(weights_path))\n",
        "        if freeze_body in [1, 2]:\n",
        "            # Freeze darknet53 body or freeze all but 3 output layers.\n",
        "            num = (185, len(model_body.layers)-3)[freeze_body-1]\n",
        "            for i in range(num): model_body.layers[i].trainable = False\n",
        "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
        "\n",
        "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
        "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})(\n",
        "        [*model_body.output, *y_true])\n",
        "    model = Model([model_body.input, *y_true], model_loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_tiny_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
        "            weights_path='model_data/tiny_yolo_weights.h5'):\n",
        "    '''create the training model, for Tiny YOLOv3'''\n",
        "    K.clear_session() # get a new session\n",
        "    image_input = Input(shape=(None, None, 3))\n",
        "    h, w = input_shape\n",
        "    num_anchors = len(anchors)\n",
        "\n",
        "    y_true = [Input(shape=(h//{0:32, 1:16}[l], w//{0:32, 1:16}[l], \\\n",
        "        num_anchors//2, num_classes+5)) for l in range(2)]\n",
        "\n",
        "    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n",
        "    print('Create Tiny YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
        "\n",
        "    if load_pretrained:\n",
        "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
        "        print('Load weights {}.'.format(weights_path))\n",
        "        if freeze_body in [1, 2]:\n",
        "            # Freeze the darknet body or freeze all but 2 output layers.\n",
        "            num = (20, len(model_body.layers)-2)[freeze_body-1]\n",
        "            for i in range(num): model_body.layers[i].trainable = False\n",
        "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
        "\n",
        "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
        "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.7})(\n",
        "        [*model_body.output, *y_true])\n",
        "    model = Model([model_body.input, *y_true], model_loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
        "    '''data generator for fit_generator'''\n",
        "    n = len(annotation_lines)\n",
        "    i = 0\n",
        "    while True:\n",
        "        image_data = []\n",
        "        box_data = []\n",
        "        for b in range(batch_size):\n",
        "            if i==0:\n",
        "                np.random.shuffle(annotation_lines)\n",
        "            image, box = get_random_data(annotation_lines[i], input_shape, random=True)\n",
        "            image_data.append(image)\n",
        "            box_data.append(box)\n",
        "            i = (i+1) % n\n",
        "        image_data = np.array(image_data)\n",
        "        box_data = np.array(box_data)\n",
        "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
        "        yield [image_data, *y_true], np.zeros(batch_size)\n",
        "\n",
        "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
        "    n = len(annotation_lines)\n",
        "    if n==0 or batch_size<=0: return None\n",
        "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    _main()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'Iterable' from 'collections' (/usr/lib/python3.10/collections/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-2ae0e5908328>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmulti_gpu_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmulti_gpu_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/multi_gpu_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_source_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstandardize_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mweighted_masked_objective\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_num_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProgbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProgbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Iterable' from 'collections' (/usr/lib/python3.10/collections/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48yw4UaOYgQS"
      },
      "source": [
        "## can call this cell instead of the above\n",
        "# !python train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFX-2_M8bMQ3"
      },
      "source": [
        "## Use our model for inference\n",
        "\n",
        "For predictions, we'll call a a Python script called `yolo_video.py` with required arguments for our use case: a path to our specific first stage trained weights (see our blog for why we're using only stage one), a path to our custom class names, and a flag to specify we're using images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlVyevd8b8gG"
      },
      "source": [
        "Additional arguments for `yolo_video.py` are as follows:\n",
        "\n",
        "```\n",
        "usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]\n",
        "                     [--classes CLASSES] [--gpu_num GPU_NUM] [--image]\n",
        "                     [--input] [--output]\n",
        "\n",
        "positional arguments:\n",
        "  --input        Video input path\n",
        "  --output       Video output path\n",
        "\n",
        "optional arguments:\n",
        "  -h, --help         show this help message and exit\n",
        "  --model MODEL      path to model weight file, default model_data/yolo.h5\n",
        "  --anchors ANCHORS  path to anchor definitions, default\n",
        "                     model_data/yolo_anchors.txt\n",
        "  --classes CLASSES  path to class definitions, default\n",
        "                     model_data/coco_classes.txt\n",
        "  --gpu_num GPU_NUM  Number of GPU to use, default 1\n",
        "  --image            Image detection mode, will ignore all positional arguments\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcJbmgNEO1bE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "802c1b7e-4f76-4fed-be3b-f53136eb2956"
      },
      "source": [
        "!python yolo_video.py --model=\"./logs/000/trained_weights_stage_1.h5\" --classes=\"_classes.txt\" --image"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend.\n",
            "2024-04-01 11:47:29.697806: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-01 11:47:29.697855: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-01 11:47:29.699036: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-01 11:47:30.661053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/keras-yolo3/yolo_video.py\", line 3, in <module>\n",
            "    from yolo import YOLO, detect_video\n",
            "  File \"/content/keras-yolo3/yolo.py\", line 11, in <module>\n",
            "    from keras import backend as K\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/__init__.py\", line 3, in <module>\n",
            "    from . import utils\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/__init__.py\", line 27, in <module>\n",
            "    from .multi_gpu_utils import multi_gpu_model\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/multi_gpu_utils.py\", line 7, in <module>\n",
            "    from ..layers.merge import concatenate\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/layers/__init__.py\", line 4, in <module>\n",
            "    from ..engine.base_layer import Layer\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/__init__.py\", line 8, in <module>\n",
            "    from .training import Model\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 21, in <module>\n",
            "    from . import training_arrays\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training_arrays.py\", line 14, in <module>\n",
            "    from .. import callbacks as cbks\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\", line 20, in <module>\n",
            "    from collections import Iterable\n",
            "ImportError: cannot import name 'Iterable' from 'collections' (/usr/lib/python3.10/collections/__init__.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbACT_RJdGVg"
      },
      "source": [
        "For input image names into the above, consider trying the following:\n",
        "\n",
        "- `00a7a49c47d51fd16a4cbb17e2d2cf86.jpg` # white-king works! + knight\n",
        "- `015d0d7ff365f0b7492ff079c8c7d56c.jpg` # black-queen mixes up\n",
        "- `176b28b5c417f39a9e5d37545fca5b4c.jpg` # finds only five\n",
        "- `4673f994f60a2ea7afdddc1b752947c0.jpg` # white-rook (thinks king)\n",
        "- `5ca7f0cb1c500554e65ad031190f8e9f.jpg` # white-pawn (missed white-king)\n",
        "- `fbf15139f38a46e02b5f4061c0c9b08f.jpg` # black-king success!\n",
        "\n",
        "You can view these images in your Colab notebook by clicking on the image name in the expanded left-hand panel (Files → keras-yolo3 → IMG_NAME )."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88oJlBl4dumo"
      },
      "source": [
        "## Move currently trained model to GDrive\n",
        "\n",
        "Optionally, you may want to save the new weights that your model trained so that the next time you run this notebook, you can either skip training and use these weights for inference or begin training where you left off with this weights file.\n",
        "\n",
        "Following the below will link your Colab notebook to your Google Drive, and save the weights (named as the current time you saved them to enforce a unique file name) in your Drive folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4t94dBNdsxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc1dac9e-6fad-49cd-e799-7c2e7f49de48"
      },
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLe0Y4Z8BOVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e41ecf9-8246-49b7-db2e-f662dc5b39cd"
      },
      "source": [
        "# create a copy of the weights file with a datetime\n",
        "# and move that file to your own Drive\n",
        "%cp ./logs/000/trained_weights_stage_1.h5 ./logs/000/trained_weights_stage_1_$(date +%F-%H:%M).h5\n",
        "%mv ./logs/000/trained_weights_stage_1_$(date +%F-%H:%M).h5 /content/drive/My\\ Drive/"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat './logs/000/trained_weights_stage_1.h5': No such file or directory\n",
            "mv: cannot stat './logs/000/trained_weights_stage_1_2024-04-01-11:48.h5': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "84JSNdZAfHgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problems encountered:\n",
        "\n",
        "- The problem I encounter in this activity is that Google Colab only allows tensorflow version 2.x while YOLOv3 Keras needs the tensorflow version 1.x so I can't train my custom dataset using YOLOv3 Keras."
      ],
      "metadata": {
        "id": "CATN7dDrfQlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gJ27IlkwfUqW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}